{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1mZ1BFp-iE9BJuCek_D34mJzd5IDt7P3t",
      "authorship_tag": "ABX9TyN8DrJSYerJOyoydziXddEC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineetsalar88/ResrachPaper2/blob/master/18Dec25Resnet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dataset/\n",
        " ‚îî‚îÄ‚îÄ images/\n",
        "     ‚îú‚îÄ‚îÄ img001.jpg\n",
        "     ‚îú‚îÄ‚îÄ img002.jpg\n",
        "     ‚îú‚îÄ‚îÄ img003.jpg\n",
        "     ‚îî‚îÄ‚îÄ ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "09gSH04IeH7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "image_name,label\n",
        "img001.jpg,2\n",
        "img002.jpg,0\n",
        "img003.jpg,6\n"
      ],
      "metadata": {
        "id": "5MxaHK_53rDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WfuXz3DbWIpH"
      },
      "outputs": [],
      "source": [
        "#Custom Dataset to read images + CSV\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImageCSVDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.iloc[idx, 0]\n",
        "        label = int(self.data.iloc[idx, 1])\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ],
      "metadata": {
        "id": "uwKozq_G1-x9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsResnet.csv\")\n",
        "\n",
        "class_map = {\n",
        "    \"Score 1\": 0,\n",
        "    \"Score 2\": 1,\n",
        "    \"Score 3\": 2,\n",
        "    \"Score 4\": 3,\n",
        "    \"Score 5\": 4,\n",
        "    \"Score 6\": 5,\n",
        "    \"Score 7\": 6\n",
        "}\n",
        "\n",
        "df[\"label\"] = df[\"label\"].map(class_map)\n",
        "df.to_csv(\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsImagenet.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Pup7oLGtzWV6",
        "outputId": "8f068f58-ca0e-487a-a6e1-eeffe710a9cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsResnet.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3811267891.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsResnet.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m class_map = {\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsResnet.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Dataset & DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = ImageCSVDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsImagenet.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/Images\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "FSyOtUOScPV1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load ResNet18 & freeze feature extractor\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# üîí Freeze all convolution layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pxZqK9ucmGO",
        "outputId": "666f51cd-51a7-4569-a350-21f5b7082314"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace classifier for 7 classes\n",
        "num_classes = 7\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.requires_grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-pDTm7yc3m4",
        "outputId": "af48c649-5c8c-45f5-80d3-588bb3cfb1ed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight False\n",
            "bn1.weight False\n",
            "bn1.bias False\n",
            "layer1.0.conv1.weight False\n",
            "layer1.0.bn1.weight False\n",
            "layer1.0.bn1.bias False\n",
            "layer1.0.conv2.weight False\n",
            "layer1.0.bn2.weight False\n",
            "layer1.0.bn2.bias False\n",
            "layer1.1.conv1.weight False\n",
            "layer1.1.bn1.weight False\n",
            "layer1.1.bn1.bias False\n",
            "layer1.1.conv2.weight False\n",
            "layer1.1.bn2.weight False\n",
            "layer1.1.bn2.bias False\n",
            "layer2.0.conv1.weight False\n",
            "layer2.0.bn1.weight False\n",
            "layer2.0.bn1.bias False\n",
            "layer2.0.conv2.weight False\n",
            "layer2.0.bn2.weight False\n",
            "layer2.0.bn2.bias False\n",
            "layer2.0.downsample.0.weight False\n",
            "layer2.0.downsample.1.weight False\n",
            "layer2.0.downsample.1.bias False\n",
            "layer2.1.conv1.weight False\n",
            "layer2.1.bn1.weight False\n",
            "layer2.1.bn1.bias False\n",
            "layer2.1.conv2.weight False\n",
            "layer2.1.bn2.weight False\n",
            "layer2.1.bn2.bias False\n",
            "layer3.0.conv1.weight False\n",
            "layer3.0.bn1.weight False\n",
            "layer3.0.bn1.bias False\n",
            "layer3.0.conv2.weight False\n",
            "layer3.0.bn2.weight False\n",
            "layer3.0.bn2.bias False\n",
            "layer3.0.downsample.0.weight False\n",
            "layer3.0.downsample.1.weight False\n",
            "layer3.0.downsample.1.bias False\n",
            "layer3.1.conv1.weight False\n",
            "layer3.1.bn1.weight False\n",
            "layer3.1.bn1.bias False\n",
            "layer3.1.conv2.weight False\n",
            "layer3.1.bn2.weight False\n",
            "layer3.1.bn2.bias False\n",
            "layer4.0.conv1.weight False\n",
            "layer4.0.bn1.weight False\n",
            "layer4.0.bn1.bias False\n",
            "layer4.0.conv2.weight False\n",
            "layer4.0.bn2.weight False\n",
            "layer4.0.bn2.bias False\n",
            "layer4.0.downsample.0.weight False\n",
            "layer4.0.downsample.1.weight False\n",
            "layer4.0.downsample.1.bias False\n",
            "layer4.1.conv1.weight False\n",
            "layer4.1.bn1.weight False\n",
            "layer4.1.bn1.bias False\n",
            "layer4.1.conv2.weight False\n",
            "layer4.1.bn2.weight False\n",
            "layer4.1.bn2.bias False\n",
            "fc.weight True\n",
            "fc.bias True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss & optimizer (only classifier trained)\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.fc.parameters(),  # üëà only classifier\n",
        "    lr=1e-3\n",
        ")\n"
      ],
      "metadata": {
        "id": "HUp-lBJMdMOZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=0,   # üî¥ ADD HERE (for debugging)\n",
        "    pin_memory=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "gTafWKXXce8M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training loop\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] Loss: {running_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz6nv0opdr8F",
        "outputId": "ef3f1b28-be79-4f84-fde7-27b4adb1b5f1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Loss: 1.6793\n",
            "Epoch [2/10] Loss: 1.4014\n",
            "Epoch [3/10] Loss: 1.2538\n",
            "Epoch [4/10] Loss: 1.1371\n",
            "Epoch [5/10] Loss: 1.0644\n",
            "Epoch [6/10] Loss: 1.0380\n",
            "Epoch [7/10] Loss: 0.9960\n",
            "Epoch [8/10] Loss: 0.9790\n",
            "Epoch [9/10] Loss: 0.9320\n",
            "Epoch [10/10] Loss: 0.8945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Part               | Status             |\n",
        "| ------------------ | ------------------ |\n",
        "| ResNet Conv layers | ‚ùÑÔ∏è Frozen          |\n",
        "| BatchNorm          | ‚ùÑÔ∏è Frozen          |\n",
        "| FC layer           | üî• Trainable       |\n",
        "| Optimizer          | FC parameters only |\n"
      ],
      "metadata": {
        "id": "2NJF5hSVd7Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optional: Fine-tuning later (advanced)\n",
        "#After a few epochs, you can unfreeze last layer block:\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "#Then reduce learning rate (1e-4)"
      ],
      "metadata": {
        "id": "vT-Xu1-vd-CN"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}