{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/vineetsalar88/ResrachPaper2/blob/master/18Dec25Resnet18.ipynb",
      "authorship_tag": "ABX9TyMjEMkdnrpXDE9ePSNnI68f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineetsalar88/ResrachPaper2/blob/master/21Dec25Resnet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsResnet.csv\")\n",
        "\n",
        "class_map = {\n",
        "    \"Score 1\": 0,\n",
        "    \"Score 2\": 1,\n",
        "    \"Score 3\": 2,\n",
        "    \"Score 4\": 3,\n",
        "    \"Score 5\": 4,\n",
        "    \"Score 6\": 5,\n",
        "    \"Score 7\": 6\n",
        "}\n",
        "\n",
        "df[\"label\"] = df[\"label\"].map(class_map)\n",
        "df.to_csv(\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsImagenet.csv\", index=False)"
      ],
      "metadata": {
        "id": "1wnp7JqMgA4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfuXz3DbWIpH"
      },
      "outputs": [],
      "source": [
        "#Custom Dataset to read images + CSV\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImageCSVDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.iloc[idx, 0]\n",
        "        label = int(self.data.iloc[idx, 1])\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XKq75inILVy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 128x128\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "dataset = ImageCSVDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsImagenet.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/Images\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "dataset_size = len(dataset)  # Assuming 'dataset' is your ImageFolder dataset\n",
        "train_size = int(0.8 * dataset_size)  # 80% for training\n",
        "val_size = int(0.2 * dataset_size)  # 20% for validation\n",
        "#test_size = dataset_size - train_size - val_size  # Remaining 10% for testing\n",
        "\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Dataset & DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = ImageCSVDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsImagenet.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/Images\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "FSyOtUOScPV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Part               | Status             |\n",
        "| ------------------ | ------------------ |\n",
        "| ResNet Conv layers | ‚ùÑÔ∏è Frozen          |\n",
        "| BatchNorm          | ‚ùÑÔ∏è Frozen          |\n",
        "| FC layer           | üî• Trainable       |\n",
        "| Optimizer          | FC parameters only |\n"
      ],
      "metadata": {
        "id": "2NJF5hSVd7Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load ResNet18 & freeze feature extractor\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# üîí Freeze all convolution layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pxZqK9ucmGO",
        "outputId": "3edc3c5c-0891-4167-b6dc-0493e8d02421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 179MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace classifier for 7 classes\n",
        "num_classes = 7\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.requires_grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-pDTm7yc3m4",
        "outputId": "dc30d6a4-1ed3-42c0-8475-6842cd638ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight False\n",
            "bn1.weight False\n",
            "bn1.bias False\n",
            "layer1.0.conv1.weight False\n",
            "layer1.0.bn1.weight False\n",
            "layer1.0.bn1.bias False\n",
            "layer1.0.conv2.weight False\n",
            "layer1.0.bn2.weight False\n",
            "layer1.0.bn2.bias False\n",
            "layer1.1.conv1.weight False\n",
            "layer1.1.bn1.weight False\n",
            "layer1.1.bn1.bias False\n",
            "layer1.1.conv2.weight False\n",
            "layer1.1.bn2.weight False\n",
            "layer1.1.bn2.bias False\n",
            "layer2.0.conv1.weight False\n",
            "layer2.0.bn1.weight False\n",
            "layer2.0.bn1.bias False\n",
            "layer2.0.conv2.weight False\n",
            "layer2.0.bn2.weight False\n",
            "layer2.0.bn2.bias False\n",
            "layer2.0.downsample.0.weight False\n",
            "layer2.0.downsample.1.weight False\n",
            "layer2.0.downsample.1.bias False\n",
            "layer2.1.conv1.weight False\n",
            "layer2.1.bn1.weight False\n",
            "layer2.1.bn1.bias False\n",
            "layer2.1.conv2.weight False\n",
            "layer2.1.bn2.weight False\n",
            "layer2.1.bn2.bias False\n",
            "layer3.0.conv1.weight False\n",
            "layer3.0.bn1.weight False\n",
            "layer3.0.bn1.bias False\n",
            "layer3.0.conv2.weight False\n",
            "layer3.0.bn2.weight False\n",
            "layer3.0.bn2.bias False\n",
            "layer3.0.downsample.0.weight False\n",
            "layer3.0.downsample.1.weight False\n",
            "layer3.0.downsample.1.bias False\n",
            "layer3.1.conv1.weight False\n",
            "layer3.1.bn1.weight False\n",
            "layer3.1.bn1.bias False\n",
            "layer3.1.conv2.weight False\n",
            "layer3.1.bn2.weight False\n",
            "layer3.1.bn2.bias False\n",
            "layer4.0.conv1.weight False\n",
            "layer4.0.bn1.weight False\n",
            "layer4.0.bn1.bias False\n",
            "layer4.0.conv2.weight False\n",
            "layer4.0.bn2.weight False\n",
            "layer4.0.bn2.bias False\n",
            "layer4.0.downsample.0.weight False\n",
            "layer4.0.downsample.1.weight False\n",
            "layer4.0.downsample.1.bias False\n",
            "layer4.1.conv1.weight False\n",
            "layer4.1.bn1.weight False\n",
            "layer4.1.bn1.bias False\n",
            "layer4.1.conv2.weight False\n",
            "layer4.1.bn2.weight False\n",
            "layer4.1.bn2.bias False\n",
            "fc.weight True\n",
            "fc.bias True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss & optimizer (only classifier trained)\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.fc.parameters(),  # üëà only classifier\n",
        "    lr=1e-3\n",
        ")\n"
      ],
      "metadata": {
        "id": "HUp-lBJMdMOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 10\n",
        "checkpoint_path = \"/content/drive/MyDrive/models/resnet18_checkpoint.pth\"\n",
        "\n",
        "# ======================\n",
        "# üîπ LOAD CHECKPOINT\n",
        "# ======================\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    start_epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "    train_losses = checkpoint[\"train_losses\"]\n",
        "    train_accuracies = checkpoint[\"train_accuracies\"]\n",
        "\n",
        "    print(f\"‚úÖ Resuming from epoch {start_epoch}\")\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "# ======================\n",
        "# üîπ TRAINING LOOP\n",
        "# ======================\n",
        "for epoch in range(start_epoch, epochs):\n",
        "\n",
        "    # -------- TRAIN --------\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100.0 * train_correct / train_total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # -------- VALIDATION --------\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100.0 * correct / total\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(val_acc)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "        f\"Train Loss: {train_loss:.4f} | \"\n",
        "        f\"Train Acc: {train_acc:.2f}% | \"\n",
        "        f\"Val Loss: {val_loss:.4f} | \"\n",
        "        f\"Val Acc: {val_acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "    # ======================\n",
        "    # üîπ SAVE CHECKPOINT\n",
        "    # ======================\n",
        "    torch.save({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_accuracies\": train_accuracies\n",
        "    }, checkpoint_path)\n"
      ],
      "metadata": {
        "id": "ivIIYQ3FD_nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_range = range(1, len(train_accuracies) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs_range, train_accuracies, marker='o', label=\"Train Accuracy\")\n",
        "plt.plot(epochs_range, val_accuracies, marker='o', label=\"Validation Accuracy\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8m2D2hsSJKTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(cm)\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\"\n",
        ")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hHEEjzSeJRJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}