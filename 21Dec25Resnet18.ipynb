{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/vineetsalar88/ResrachPaper2/blob/master/18Dec25Resnet18.ipynb",
      "authorship_tag": "ABX9TyOjSPntdwOJWcFgcb4TDzJ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineetsalar88/ResrachPaper2/blob/master/21Dec25Resnet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsResnet.csv\")\n",
        "\n",
        "class_map = {\n",
        "    \"Score 1\": 0,\n",
        "    \"Score 2\": 1,\n",
        "    \"Score 3\": 2,\n",
        "    \"Score 4\": 3,\n",
        "    \"Score 5\": 4,\n",
        "    \"Score 6\": 5,\n",
        "    \"Score 7\": 6\n",
        "}\n",
        "\n",
        "df[\"label\"] = df[\"label\"].map(class_map)\n",
        "df.to_csv(\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsImagenet.csv\", index=False)"
      ],
      "metadata": {
        "id": "1wnp7JqMgA4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WfuXz3DbWIpH"
      },
      "outputs": [],
      "source": [
        "#Custom Dataset to read images + CSV\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImageCSVDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.iloc[idx, 0]\n",
        "        label = int(self.data.iloc[idx, 1])\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3XKq75inILVy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 128x128\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "dataset = ImageCSVDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsImagenet.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/Images\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "dataset_size = len(dataset)  # Assuming 'dataset' is your ImageFolder dataset\n",
        "train_size = int(0.8 * dataset_size)  # 80% for training\n",
        "val_size = int(0.1 * dataset_size)  # 10% for validation\n",
        "test_size = dataset_size - train_size - val_size  # Remaining 10% for testing\n",
        "\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Dataset & DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = ImageCSVDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/annotationsImagenet.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/ResearchData/AnnonatedDataset17Dec25/Images\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "FSyOtUOScPV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load ResNet18 & freeze feature extractor\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# ðŸ”’ Freeze all convolution layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pxZqK9ucmGO",
        "outputId": "3edc3c5c-0891-4167-b6dc-0493e8d02421"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 179MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace classifier for 7 classes\n",
        "num_classes = 7\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.requires_grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-pDTm7yc3m4",
        "outputId": "dc30d6a4-1ed3-42c0-8475-6842cd638ceb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight False\n",
            "bn1.weight False\n",
            "bn1.bias False\n",
            "layer1.0.conv1.weight False\n",
            "layer1.0.bn1.weight False\n",
            "layer1.0.bn1.bias False\n",
            "layer1.0.conv2.weight False\n",
            "layer1.0.bn2.weight False\n",
            "layer1.0.bn2.bias False\n",
            "layer1.1.conv1.weight False\n",
            "layer1.1.bn1.weight False\n",
            "layer1.1.bn1.bias False\n",
            "layer1.1.conv2.weight False\n",
            "layer1.1.bn2.weight False\n",
            "layer1.1.bn2.bias False\n",
            "layer2.0.conv1.weight False\n",
            "layer2.0.bn1.weight False\n",
            "layer2.0.bn1.bias False\n",
            "layer2.0.conv2.weight False\n",
            "layer2.0.bn2.weight False\n",
            "layer2.0.bn2.bias False\n",
            "layer2.0.downsample.0.weight False\n",
            "layer2.0.downsample.1.weight False\n",
            "layer2.0.downsample.1.bias False\n",
            "layer2.1.conv1.weight False\n",
            "layer2.1.bn1.weight False\n",
            "layer2.1.bn1.bias False\n",
            "layer2.1.conv2.weight False\n",
            "layer2.1.bn2.weight False\n",
            "layer2.1.bn2.bias False\n",
            "layer3.0.conv1.weight False\n",
            "layer3.0.bn1.weight False\n",
            "layer3.0.bn1.bias False\n",
            "layer3.0.conv2.weight False\n",
            "layer3.0.bn2.weight False\n",
            "layer3.0.bn2.bias False\n",
            "layer3.0.downsample.0.weight False\n",
            "layer3.0.downsample.1.weight False\n",
            "layer3.0.downsample.1.bias False\n",
            "layer3.1.conv1.weight False\n",
            "layer3.1.bn1.weight False\n",
            "layer3.1.bn1.bias False\n",
            "layer3.1.conv2.weight False\n",
            "layer3.1.bn2.weight False\n",
            "layer3.1.bn2.bias False\n",
            "layer4.0.conv1.weight False\n",
            "layer4.0.bn1.weight False\n",
            "layer4.0.bn1.bias False\n",
            "layer4.0.conv2.weight False\n",
            "layer4.0.bn2.weight False\n",
            "layer4.0.bn2.bias False\n",
            "layer4.0.downsample.0.weight False\n",
            "layer4.0.downsample.1.weight False\n",
            "layer4.0.downsample.1.bias False\n",
            "layer4.1.conv1.weight False\n",
            "layer4.1.bn1.weight False\n",
            "layer4.1.bn1.bias False\n",
            "layer4.1.conv2.weight False\n",
            "layer4.1.bn2.weight False\n",
            "layer4.1.bn2.bias False\n",
            "fc.weight True\n",
            "fc.bias True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss & optimizer (only classifier trained)\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    model.fc.parameters(),  # ðŸ‘ˆ only classifier\n",
        "    lr=1e-3\n",
        ")\n"
      ],
      "metadata": {
        "id": "HUp-lBJMdMOZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # ======================\n",
        "    # ðŸ”¹ TRAINING\n",
        "    # ======================\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # ======================\n",
        "    # ðŸ”¹ VALIDATION\n",
        "    # ======================\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100.0 * correct / total\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "        f\"Train Loss: {train_loss:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f} | \"\n",
        "        f\"Val Acc: {val_acc:.2f}%\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "rShBnZL1o-Sy",
        "outputId": "e90a0b3c-2b83-4c4f-b58d-8a7decc04bbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Train Loss: 0.8356 | Val Loss: 1.1772 | Val Acc: 46.38%\n",
            "Epoch [2/10] Train Loss: 0.8788 | Val Loss: 1.1451 | Val Acc: 55.07%\n",
            "Epoch [3/10] Train Loss: 0.8209 | Val Loss: 1.0682 | Val Acc: 53.62%\n",
            "Epoch [4/10] Train Loss: 0.8227 | Val Loss: 1.0556 | Val Acc: 57.97%\n",
            "Epoch [5/10] Train Loss: 0.7392 | Val Loss: 1.0352 | Val Acc: 59.42%\n",
            "Epoch [6/10] Train Loss: 0.7265 | Val Loss: 1.1459 | Val Acc: 50.72%\n",
            "Epoch [7/10] Train Loss: 0.7088 | Val Loss: 0.9929 | Val Acc: 62.32%\n",
            "Epoch [8/10] Train Loss: 0.6961 | Val Loss: 0.9987 | Val Acc: 60.87%\n",
            "Epoch [9/10] Train Loss: 0.7389 | Val Loss: 1.0341 | Val Acc: 56.52%\n",
            "Epoch [10/10] Train Loss: 0.6932 | Val Loss: 1.0254 | Val Acc: 57.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Part               | Status             |\n",
        "| ------------------ | ------------------ |\n",
        "| ResNet Conv layers | â„ï¸ Frozen          |\n",
        "| BatchNorm          | â„ï¸ Frozen          |\n",
        "| FC layer           | ðŸ”¥ Trainable       |\n",
        "| Optimizer          | FC parameters only |\n"
      ],
      "metadata": {
        "id": "2NJF5hSVd7Ke"
      }
    }
  ]
}